{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alysha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as srec\n",
    "from gtts import gTTS\n",
    "import pyttsx3 as pyt\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import time\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = pyt.init()\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id)\n",
    "\n",
    "device = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AZURE SAILOR 0.5B MODEL ---\n",
    "start_time = time.time()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'sail/Sailor-0.5B-Chat',\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('sail/Sailor-0.5B-Chat')\n",
    "end_time = time.time()\n",
    "\n",
    "load_time = end_time - start_time\n",
    "print(load_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLAMA ---\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SeaLLMs-v3-7B-Chat ---\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    " \"SeaLLMs/SeaLLMs-v3-1.5B-Chat\", # can change to \"SeaLLMs/SeaLLMs-v3-1.5B-Chat\" if your resource is limited\n",
    "  torch_dtype=torch.bfloat16, \n",
    "  device_map='cpu'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SeaLLMs/SeaLLMs-v3-1.5B-Chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STT ---\n",
    "\n",
    "def perintah():\n",
    "    mendengar = srec.Recognizer()\n",
    "    with srec.Microphone() as source:\n",
    "        print('Mendengarkan......')\n",
    "        suara = mendengar.listen(source, phrase_time_limit=5)\n",
    "        try:\n",
    "            print('Diterima.....')\n",
    "            dengar = mendengar.recognize_google(suara, language='id-ID')\n",
    "            print(dengar)\n",
    "        except:\n",
    "            pass\n",
    "        return dengar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TTS ---\n",
    "\n",
    "def ngomong(text):\n",
    "    voices = engine.getProperty('voices')\n",
    "\n",
    "    for voice in voices:\n",
    "        if \"MSTTS_V110_idID_Andika\" in voice.id:\n",
    "            engine.setProperty('voice', voice.id)\n",
    "            break\n",
    "\n",
    "    # Speak the text\n",
    "    engine.say(text)\n",
    "    \n",
    "    # Wait until speaking is finished\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_va():\n",
    "    # prompt from user\n",
    "    Layanan = perintah()\n",
    "\n",
    "    # messages = [{\"role\": \"user\", \"content\": Layanan}]\n",
    "    # prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "    # print(outputs[0][\"generated_text\"])\n",
    "\n",
    "    # ngomong(Layanan)\n",
    "\n",
    "    # print(Layanan)\n",
    "\n",
    "    system_prompt= 'Jawab dalam bahasa indonesia.'\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"question\", \"content\": Layanan}\n",
    "        ]\n",
    "\n",
    "    # text = tokenizer.apply_chat_template(\n",
    "    #     messages,\n",
    "    #     tokenize=False,\n",
    "    #     add_generation_prompt=True\n",
    "    # )\n",
    "\n",
    "    model_inputs = tokenizer(Layanan, return_tensors=\"pt\") \n",
    "\n",
    "    start_time = time.time()\n",
    "    memory_before = psutil.virtual_memory().used\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    end_time = time.time()\n",
    "    memory_after = psutil.virtual_memory().used\n",
    "\n",
    "    inference_time = end_time - start_time\n",
    "    memory_used = memory_after - memory_before\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)\n",
    "    \n",
    "    print(response)\n",
    "    print(\"INFERENCE TIME: \", inference_time)\n",
    "    print(\"MEMORY USAGE: \", memory_used)\n",
    "    print(\"CPU USAGE: \", cpu_usage)\n",
    "    # ngomong(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_va():\n",
    "    # prompt from user\n",
    "    Layanan = perintah()\n",
    "\n",
    "    # messages = [{\"role\": \"user\", \"content\": Layanan}]\n",
    "    # prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "    # print(outputs[0][\"generated_text\"])\n",
    "\n",
    "    # ngomong(Layanan)\n",
    "\n",
    "    # print(Layanan)\n",
    "\n",
    "    system_prompt= 'Jawab dalam bahasa indonesia.'\n",
    "\n",
    "    # messages = [\n",
    "    #     {\"role\": \"question\", \"content\": Layanan}\n",
    "    #     ]\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Tolong jawab singkat.\"},\n",
    "    {\"role\": \"user\", \"content\": Layanan}\n",
    "]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\") \n",
    "\n",
    "    start_time = time.time()\n",
    "    memory_before = psutil.virtual_memory().used\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512, \n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    end_time = time.time()\n",
    "    memory_after = psutil.virtual_memory().used\n",
    "\n",
    "    inference_time = end_time - start_time\n",
    "    memory_used = memory_after - memory_before\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)\n",
    "    \n",
    "    print(response)\n",
    "    print(\"INFERENCE TIME: \", inference_time)\n",
    "    print(\"MEMORY USAGE: \", memory_used)\n",
    "    print(\"CPU USAGE: \", cpu_usage)\n",
    "    ngomong(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mendengarkan......\n",
      "Diterima.....\n",
      "Apa yang akan terjadi besok\n",
      "Maaf, sebagai AI, saya tidak memiliki kemampuan untuk memprediksi masa depan dengan pasti. Prediksi masa depan sangat bergantung pada berbagai faktor seperti kondisi alamiah, kebijakan pemerintah, dan perubahan sosial-ekonomi. Jika Anda mencari informasi tentang perkiraan cuaca atau peristiwa yang akan terjadi di masa depan, saya bisa membantu memberikan gambaran umum berdasarkan data yang tersedia hingga batas pengetahuan saya.\n",
      "INFERENCE TIME:  192.39053440093994\n",
      "MEMORY USAGE:  175443968\n",
      "CPU USAGE:  5.1\n"
     ]
    }
   ],
   "source": [
    "run_va()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
